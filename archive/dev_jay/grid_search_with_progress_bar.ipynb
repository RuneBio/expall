{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Jay\\Miniconda3\\lib\\importlib\\_bootstrap.py:219: ImportWarning: can't resolve package from __spec__ or __package__, falling back on __name__ and __path__\n",
      "  return f(*args, **kwds)\n",
      "C:\\Users\\Jay\\Miniconda3\\lib\\importlib\\_bootstrap.py:219: ImportWarning: can't resolve package from __spec__ or __package__, falling back on __name__ and __path__\n",
      "  return f(*args, **kwds)\n",
      "C:\\Users\\Jay\\Miniconda3\\lib\\site-packages\\h5py\\__init__.py:36: FutureWarning: Conversion of the second argument of issubdtype from `float` to `np.floating` is deprecated. In future, it will be treated as `np.float64 == np.dtype(float).type`.\n",
      "  from ._conv import register_converters as _register_converters\n",
      "Using TensorFlow backend.\n",
      "C:\\Users\\Jay\\Miniconda3\\lib\\importlib\\_bootstrap_external.py:426: ImportWarning: Not importing directory C:\\Users\\Jay\\Miniconda3\\lib\\site-packages\\google: missing __init__\n",
      "  _warnings.warn(msg.format(portions[0]), ImportWarning)\n",
      "C:\\Users\\Jay\\Miniconda3\\lib\\site-packages\\tensorflow\\python\\framework\\ops.py:875: DeprecationWarning: builtin type EagerTensor has no __module__ attribute\n",
      "  EagerTensor = c_api.TFE_Py_InitEagerTensor(_EagerTensorBase)\n",
      "C:\\Users\\Jay\\Miniconda3\\lib\\site-packages\\tensorflow\\python\\util\\tf_inspect.py:45: DeprecationWarning: inspect.getargspec() is deprecated, use inspect.signature() or inspect.getfullargspec()\n",
      "  if d.decorator_argspec is not None), _inspect.getargspec(target))\n",
      "C:\\Users\\Jay\\Miniconda3\\lib\\site-packages\\tensorflow\\python\\keras\\_impl\\keras\\backend.py:4622: ResourceWarning: unclosed file <_io.TextIOWrapper name='C:\\\\Users\\\\Jay\\\\.keras\\\\keras.json' mode='r' encoding='cp1252'>\n",
      "  _config = json.load(open(_config_path))\n",
      "C:\\Users\\Jay\\Miniconda3\\lib\\site-packages\\tensorflow\\python\\util\\tf_inspect.py:45: DeprecationWarning: inspect.getargspec() is deprecated, use inspect.signature() or inspect.getfullargspec()\n",
      "  if d.decorator_argspec is not None), _inspect.getargspec(target))\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "from keras.preprocessing.text import Tokenizer\n",
    "from sklearn.model_selection import train_test_split\n",
    "from keras.preprocessing import sequence\n",
    "\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Dense\n",
    "from keras.layers import LSTM\n",
    "from keras.layers.convolutional import Conv1D\n",
    "from keras.layers.convolutional import MaxPooling1D\n",
    "from keras.layers import Dropout\n",
    "from keras.layers.embeddings import Embedding\n",
    "\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "from keras.wrappers.scikit_learn import KerasClassifier\n",
    "\n",
    "#this is the grid search with progress bar \n",
    "# pip install git+https://github.com/pactools/pactools.git#egg=pactools\n",
    "from pactools.grid_search import GridSearchCVProgressBar "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "data shape:  (22364, 8)\n"
     ]
    }
   ],
   "source": [
    "# read original data from \n",
    "data = pd.read_csv('../../dataframes/DF_prest.csv', index_col=0)\n",
    "\n",
    "# setup 'docs' for use with Tokenizer\n",
    "def nt_seq_doc(nt_sequence):\n",
    "    if 'GACAAGCTTGCGGCCGCA' not in nt_sequence:\n",
    "        return None\n",
    "    true_nt = nt_sequence.split('GACAAGCTTGCGGCCGCA')[1]\n",
    "    if len(true_nt) % 3 != 0:\n",
    "        return None\n",
    "    return ' '.join([true_nt[i:i+3]\n",
    "                     for i in range(0, len(true_nt), 3)])\n",
    "# split quantiles\n",
    "def assign_class(conc):\n",
    "    if conc <= low_cut:\n",
    "        return 0\n",
    "    elif conc >= high_cut:\n",
    "        return 1\n",
    "    return\n",
    "\n",
    "data['nt_seq_doc'] = data['nt_seq'].apply(nt_seq_doc)\n",
    "data = data[pd.notnull(data['nt_seq_doc'])]\n",
    "\n",
    "# identify high and low classes by conc_cf quantiles\n",
    "low_cut = data['conc_cf'].quantile(0.25)\n",
    "high_cut = data['conc_cf'].quantile(0.75)\n",
    "\n",
    "data['class'] = data['conc_cf'].apply(assign_class)\n",
    "data = data[pd.notnull(data['class'])]\n",
    "# check shape\n",
    "print('data shape: ', data.shape)\n",
    "\n",
    "# define sequence documents\n",
    "docs = list(data['nt_seq_doc'])\n",
    "# create the tokenizer\n",
    "t = Tokenizer()\n",
    "# fit the tokenizer on the documents\n",
    "t.fit_on_texts(docs)\n",
    "\n",
    "# integer encode documents\n",
    "X = t.texts_to_sequences(docs)\n",
    "y = data['class'].values\n",
    "\n",
    "# create test-train split\n",
    "#X_train, X_test, y_train, y_test = train_test_split(\n",
    "#    X, y, test_size=0.3, random_state=42)\n",
    "\n",
    "# fix random seed for reproducibility\n",
    "np.random.seed(7)\n",
    "\n",
    "# load the dataset but only keep the top n words, zero the rest\n",
    "top_words = len(t.word_index) + 1\n",
    "\n",
    "# truncate and pad input sequences\n",
    "seq_lengths = [len(seq) for seq in X]\n",
    "max_seq_length = max(seq_lengths)\n",
    "#X_train = sequence.pad_sequences(X_train, maxlen=max_seq_length)\n",
    "#X_test = sequence.pad_sequences(X_test, maxlen=max_seq_length)\n",
    "X = sequence.pad_sequences(X, maxlen=max_seq_length)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# tune hyperparameters for simple model\n",
    "\n",
    "# define simple model per Yoon Kim (2014)\n",
    "def create_model(embedding_length=16, num_filters=128, pool_size=2,\n",
    "                 lstm_nodes=100, drop=0.5, recurrent_drop=0.2):\n",
    "    # create the model\n",
    "    model = Sequential()\n",
    "    model.add(Embedding(top_words, embedding_length, \n",
    "                        input_length=max_seq_length))\n",
    "    model.add(Conv1D(filters=num_filters, kernel_size=3, \n",
    "                     padding='same', activation='selu'))\n",
    "    model.add(MaxPooling1D(pool_size=pool_size))\n",
    "    model.add(LSTM(lstm_nodes, dropout=drop, \n",
    "              recurrent_dropout=recurrent_drop))\n",
    "    model.add(Dense(1, activation='sigmoid'))\n",
    "    model.compile(loss='binary_crossentropy', optimizer='adam', \n",
    "                  metrics=['accuracy'])\n",
    "\n",
    "    return model\n",
    "\n",
    "\n",
    "model = KerasClassifier(build_fn=create_model, batch_size=64,\n",
    "                        epochs=50, verbose=2)\n",
    "# define the grid search parameters\n",
    "# model hyperparameters\n",
    "#embedding_length = [4, 8, 16]\n",
    "num_filters = [100, 200]\n",
    "#filter_length = [2, 3, 4, 5]\n",
    "pool_size = [3, 4]\n",
    "lstm_nodes = [100, 200]\n",
    "drop = [0.5, 0.8]\n",
    "recurrent_drop = [0.2, 0.5, 0.8]\n",
    "\n",
    "param_grid = dict(num_filters=num_filters, pool_size=pool_size,\n",
    "                  lstm_nodes=lstm_nodes, drop=drop, \n",
    "                  recurrent_drop=recurrent_drop)\n",
    "\n",
    "grid = GridSearchCV(estimator=model, param_grid=param_grid,\n",
    "                    cv=5, n_jobs=28)\n",
    "\n",
    "grid = GridSearchCV(estimator=model, param_grid=param_grid,\n",
    "                    cv=5, n_jobs=28)\n",
    "\n",
    "\n",
    "grid_result = grid.fit(X, y)\n",
    "# summarize results\n",
    "print(\"Best: %f using %s\" % (grid_result.best_score_, grid_result.best_params_))\n",
    "\n",
    "grid_df = pd.DataFrame(grid_result.cv_results_['params'])\n",
    "grid_df['means'] = grid_result.cv_results_['mean_test_score']\n",
    "grid_df['stddev'] = grid_result.cv_results_['std_test_score']\n",
    "\n",
    "# print results to csv file\n",
    "grid_df.to_csv('clstm_5fold_grid_search_results.csv')"
   ]
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
