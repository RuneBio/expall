{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "from keras.preprocessing.text import Tokenizer\n",
    "from sklearn.model_selection import train_test_split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "data shape:  (22364, 8)\n"
     ]
    }
   ],
   "source": [
    "# read original data from \n",
    "data = pd.read_csv('../dataframes/DF_prest.csv', index_col=0)\n",
    "\n",
    "# setup 'docs' for use with Tokenizer\n",
    "def nt_seq_doc(nt_sequence):\n",
    "    if 'GACAAGCTTGCGGCCGCA' not in nt_sequence:\n",
    "        return None\n",
    "    true_nt = nt_sequence.split('GACAAGCTTGCGGCCGCA')[1]\n",
    "    if len(true_nt) % 3 != 0:\n",
    "        return None\n",
    "    return ' '.join([true_nt[i:i+3] \n",
    "                     for i in range(0, len(true_nt), 3)])\n",
    "# split quantiles\n",
    "def assign_class(conc):\n",
    "    if conc <= low_cut:\n",
    "        return 0\n",
    "    elif conc >= high_cut:\n",
    "        return 1\n",
    "    return\n",
    "\n",
    "data['nt_seq_doc'] = data['nt_seq'].apply(nt_seq_doc)\n",
    "data = data[pd.notnull(data['nt_seq_doc'])]\n",
    "\n",
    "# identify high and low classes by conc_cf quantiles\n",
    "low_cut = data['conc_cf'].quantile(0.25)\n",
    "high_cut = data['conc_cf'].quantile(0.75)\n",
    "\n",
    "data['class'] = data['conc_cf'].apply(assign_class)\n",
    "data = data[pd.notnull(data['class'])]\n",
    "# check shape\n",
    "print('data shape: ', data.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "collapsed": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "OrderedDict([('acc', 31684), ('tac', 26551), ('tat', 22043), ('gcc', 43066), ('tgg', 21420), ('aag', 54456), ('cat', 19009), ('gag', 67563), ('ctg', 62966), ('ggc', 56405), ('tct', 27655), ('tgc', 19420), ('ccg', 10715), ('ttg', 22138), ('ccc', 55231), ('agg', 20309), ('gtg', 44810), ('atg', 34356), ('cta', 12299), ('cag', 59327), ('gct', 30054), ('tcc', 31193), ('gca', 27081), ('gcg', 32244), ('tca', 22471), ('agc', 35014), ('cct', 30135), ('gga', 27189), ('gtc', 23247), ('tta', 13479), ('cgg', 18621), ('atc', 31254), ('aac', 33661), ('aca', 26639), ('gta', 12449), ('cgc', 37768), ('ttc', 32044), ('aaa', 41705), ('act', 22748), ('aat', 31134), ('cga', 10633), ('ctc', 30427), ('cca', 30088), ('ggg', 25603), ('gtt', 18774), ('gaa', 53144), ('att', 25675), ('agt', 22614), ('ggt', 17735), ('taa', 22344), ('cac', 48286), ('tga', 22350), ('aga', 20896), ('ttt', 28701), ('acg', 10281), ('cgt', 7504), ('caa', 22951), ('ctt', 22010), ('tgt', 16982), ('ata', 13093), ('gac', 44691), ('gat', 39645), ('tcg', 7740), ('tag', 22)])\n",
      "22364\n",
      "{'gag': 1, 'ctg': 2, 'cag': 3, 'ggc': 4, 'ccc': 5, 'aag': 6, 'gaa': 7, 'cac': 8, 'gtg': 9, 'gac': 10, 'gcc': 11, 'aaa': 12, 'gat': 13, 'cgc': 14, 'agc': 15, 'atg': 16, 'aac': 17, 'gcg': 18, 'ttc': 19, 'acc': 20, 'atc': 21, 'tcc': 22, 'aat': 23, 'ctc': 24, 'cct': 25, 'cca': 26, 'gct': 27, 'ttt': 28, 'tct': 29, 'gga': 30, 'gca': 31, 'aca': 32, 'tac': 33, 'att': 34, 'ggg': 35, 'gtc': 36, 'caa': 37, 'act': 38, 'agt': 39, 'tca': 40, 'tga': 41, 'taa': 42, 'ttg': 43, 'tat': 44, 'ctt': 45, 'tgg': 46, 'aga': 47, 'agg': 48, 'tgc': 49, 'cat': 50, 'gtt': 51, 'cgg': 52, 'ggt': 53, 'tgt': 54, 'tta': 55, 'ata': 56, 'gta': 57, 'cta': 58, 'ccg': 59, 'cga': 60, 'acg': 61, 'tcg': 62, 'cgt': 63, 'tag': 64}\n",
      "{'act': 12679, 'acc': 15365, 'cca': 14321, 'aca': 13883, 'atc': 15167, 'aag': 18506, 'gtg': 17432, 'ggg': 13623, 'agt': 12626, 'tca': 12349, 'cta': 8637, 'tct': 13801, 'ttc': 15429, 'cga': 7876, 'tgg': 12293, 'gcg': 22361, 'ggt': 11081, 'cac': 22357, 'ccc': 22356, 'taa': 22339, 'gcc': 16825, 'tat': 11906, 'ccg': 7015, 'atg': 16264, 'gct': 14877, 'agg': 12145, 'tcc': 15103, 'agc': 15681, 'gga': 13772, 'att': 12787, 'aac': 15940, 'aat': 13969, 'gca': 14067, 'ctg': 18723, 'cgc': 22355, 'ggc': 22359, 'ctc': 14894, 'cct': 14409, 'tgc': 11041, 'gaa': 16860, 'tac': 13754, 'ttg': 12678, 'aaa': 15122, 'gtc': 13376, 'tta': 8060, 'tga': 22343, 'gta': 8436, 'cgg': 10514, 'gag': 19275, 'cag': 19159, 'gtt': 10802, 'cat': 11303, 'acg': 7586, 'ttt': 13908, 'gac': 17758, 'aga': 11455, 'cgt': 5931, 'gat': 15927, 'caa': 12079, 'ata': 8385, 'ctt': 12247, 'tgt': 10238, 'tcg': 5860, 'tag': 22}\n"
     ]
    }
   ],
   "source": [
    "# define sequence documents\n",
    "docs = list(data['nt_seq_doc'])\n",
    "# create the tokenizer\n",
    "t = Tokenizer()\n",
    "# fit the tokenizer on the documents\n",
    "t.fit_on_texts(docs)\n",
    "# summarize what was learned\n",
    "print(t.word_counts)\n",
    "print(t.document_count)\n",
    "print(t.word_index)\n",
    "print(t.word_docs)\n",
    "# integer encode documents\n",
    "X = t.texts_to_sequences(docs)\n",
    "y = data['class'].values\n",
    "\n",
    "# create test-train split\n",
    "X_train, X_test, y_train, y_test = train_test_split(\n",
    "    X, y, test_size=0.3, random_state=42)\n",
    "\n",
    "# fix random seed for reproducibility\n",
    "np.random.seed(7)\n",
    "\n",
    "# load the dataset but only keep the top n words, zero the rest\n",
    "top_words = len(t.word_index) + 1\n",
    "\n",
    "# truncate and pad input sequences\n",
    "seq_lengths = [len(seq) for seq in X]\n",
    "max_seq_length = max(seq_lengths)\n",
    "X_train = sequence.pad_sequences(X_train, maxlen=max_seq_length)\n",
    "X_test = sequence.pad_sequences(X_test, maxlen=max_seq_length)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Embed codons in 8 dims. Learn w/ single Long Short Term Memory Layer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "embedding_14 (Embedding)     (None, 181, 8)            520       \n",
      "_________________________________________________________________\n",
      "lstm_12 (LSTM)               (None, 100)               43600     \n",
      "_________________________________________________________________\n",
      "dense_11 (Dense)             (None, 1)                 101       \n",
      "=================================================================\n",
      "Total params: 44,221\n",
      "Trainable params: 44,221\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "None\n",
      "Epoch 1/3\n",
      "15654/15654 [==============================] - 27s 2ms/step - loss: 0.6907 - acc: 0.5378\n",
      "Epoch 2/3\n",
      "15654/15654 [==============================] - 26s 2ms/step - loss: 0.6794 - acc: 0.5719\n",
      "Epoch 3/3\n",
      "15654/15654 [==============================] - 26s 2ms/step - loss: 0.6582 - acc: 0.6122\n",
      "Accuracy: 62.00%\n"
     ]
    }
   ],
   "source": [
    "from keras.models import Sequential\n",
    "from keras.layers import Dense\n",
    "from keras.layers import LSTM\n",
    "from keras.layers.embeddings import Embedding\n",
    "from keras.preprocessing import sequence\n",
    "\n",
    "# create the model\n",
    "embedding_vecor_length = 8\n",
    "model = Sequential()\n",
    "model.add(Embedding(top_words, embedding_vecor_length, input_length=max_seq_length))\n",
    "model.add(LSTM(100))\n",
    "model.add(Dense(1, activation='sigmoid'))\n",
    "model.compile(loss='binary_crossentropy', optimizer='adam', metrics=['accuracy'])\n",
    "print(model.summary())\n",
    "model.fit(X_train, y_train, epochs=3, batch_size=100)\n",
    "# Final evaluation of the model\n",
    "scores = model.evaluate(X_test, y_test, verbose=0)\n",
    "print(\"Accuracy: %.2f%%\" % (scores[1]*100))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Add dropout to LSTM network"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "embedding_6 (Embedding)      (None, 181, 8)            520       \n",
      "_________________________________________________________________\n",
      "dropout_3 (Dropout)          (None, 181, 8)            0         \n",
      "_________________________________________________________________\n",
      "lstm_5 (LSTM)                (None, 100)               43600     \n",
      "_________________________________________________________________\n",
      "dropout_4 (Dropout)          (None, 100)               0         \n",
      "_________________________________________________________________\n",
      "dense_5 (Dense)              (None, 1)                 101       \n",
      "=================================================================\n",
      "Total params: 44,221\n",
      "Trainable params: 44,221\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "None\n",
      "Epoch 1/3\n",
      "15654/15654 [==============================] - 37s 2ms/step - loss: 0.6924 - acc: 0.5202\n",
      "Epoch 2/3\n",
      "15654/15654 [==============================] - 36s 2ms/step - loss: 0.6739 - acc: 0.5770\n",
      "Epoch 3/3\n",
      "15654/15654 [==============================] - 35s 2ms/step - loss: 0.6552 - acc: 0.6182\n",
      "Accuracy: 61.27%\n"
     ]
    }
   ],
   "source": [
    "from keras.models import Sequential\n",
    "from keras.layers import Dense\n",
    "from keras.layers import LSTM\n",
    "from keras.layers import Dropout\n",
    "from keras.layers.embeddings import Embedding\n",
    "from keras.preprocessing import sequence\n",
    "\n",
    "# create the model\n",
    "embedding_vecor_length = 8\n",
    "model = Sequential()\n",
    "model.add(Embedding(top_words, embedding_vecor_length, input_length=max_seq_length))\n",
    "model.add(Dropout(0.2))\n",
    "model.add(LSTM(100))\n",
    "model.add(Dropout(0.2))\n",
    "model.add(Dense(1, activation='sigmoid'))\n",
    "model.compile(loss='binary_crossentropy', optimizer='adam', metrics=['accuracy'])\n",
    "print(model.summary())\n",
    "model.fit(X_train, y_train, epochs=3, batch_size=64)\n",
    "# Final evaluation of the model\n",
    "scores = model.evaluate(X_test, y_test, verbose=0)\n",
    "print(\"Accuracy: %.2f%%\" % (scores[1]*100))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Hook LSTM to basic CNN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From /Users/joshsmith/Git/NovoNordisk_Capstone/.env/lib/python3.6/site-packages/tensorflow/python/util/deprecation.py:497: calling conv1d (from tensorflow.python.ops.nn_ops) with data_format=NHWC is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "`NHWC` for data_format is deprecated, use `NWC` instead\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "embedding_7 (Embedding)      (None, 181, 8)            520       \n",
      "_________________________________________________________________\n",
      "conv1d_1 (Conv1D)            (None, 181, 32)           800       \n",
      "_________________________________________________________________\n",
      "max_pooling1d_1 (MaxPooling1 (None, 90, 32)            0         \n",
      "_________________________________________________________________\n",
      "lstm_6 (LSTM)                (None, 100)               53200     \n",
      "_________________________________________________________________\n",
      "dense_6 (Dense)              (None, 1)                 101       \n",
      "=================================================================\n",
      "Total params: 54,621\n",
      "Trainable params: 54,621\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "None\n",
      "Epoch 1/3\n",
      "15654/15654 [==============================] - 21s 1ms/step - loss: 0.6867 - acc: 0.5464\n",
      "Epoch 2/3\n",
      "15654/15654 [==============================] - 19s 1ms/step - loss: 0.6478 - acc: 0.6288\n",
      "Epoch 3/3\n",
      "15654/15654 [==============================] - 19s 1ms/step - loss: 0.6396 - acc: 0.6393\n",
      "Accuracy: 63.37%\n"
     ]
    }
   ],
   "source": [
    "from keras.models import Sequential\n",
    "from keras.layers import Dense\n",
    "from keras.layers import LSTM\n",
    "from keras.layers.convolutional import Conv1D\n",
    "from keras.layers.convolutional import MaxPooling1D\n",
    "from keras.layers import Dropout\n",
    "from keras.layers.embeddings import Embedding\n",
    "from keras.preprocessing import sequence\n",
    "\n",
    "# create the model\n",
    "embedding_vecor_length = 8\n",
    "model = Sequential()\n",
    "model.add(Embedding(top_words, embedding_vecor_length, input_length=max_seq_length))\n",
    "model.add(Conv1D(filters=32, kernel_size=3, padding='same', activation='relu'))\n",
    "model.add(MaxPooling1D(pool_size=2))\n",
    "model.add(LSTM(100))\n",
    "model.add(Dense(1, activation='sigmoid'))\n",
    "model.compile(loss='binary_crossentropy', optimizer='adam', metrics=['accuracy'])\n",
    "print(model.summary())\n",
    "model.fit(X_train, y_train, epochs=3, batch_size=64)\n",
    "# Final evaluation of the model\n",
    "scores = model.evaluate(X_test, y_test, verbose=0)\n",
    "print(\"Accuracy: %.2f%%\" % (scores[1]*100))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "embedding_8 (Embedding)      (None, 181, 8)            520       \n",
      "_________________________________________________________________\n",
      "conv1d_2 (Conv1D)            (None, 181, 32)           800       \n",
      "_________________________________________________________________\n",
      "conv1d_3 (Conv1D)            (None, 181, 32)           10272     \n",
      "_________________________________________________________________\n",
      "max_pooling1d_2 (MaxPooling1 (None, 90, 32)            0         \n",
      "_________________________________________________________________\n",
      "lstm_7 (LSTM)                (None, 100)               53200     \n",
      "_________________________________________________________________\n",
      "dense_7 (Dense)              (None, 1)                 101       \n",
      "=================================================================\n",
      "Total params: 64,893\n",
      "Trainable params: 64,893\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "None\n",
      "Epoch 1/3\n",
      "15654/15654 [==============================] - 24s 2ms/step - loss: 0.6924 - acc: 0.5042\n",
      "Epoch 2/3\n",
      "15654/15654 [==============================] - 22s 1ms/step - loss: 0.6587 - acc: 0.6111\n",
      "Epoch 3/3\n",
      "15654/15654 [==============================] - 22s 1ms/step - loss: 0.6385 - acc: 0.6379\n",
      "Accuracy: 62.30%\n"
     ]
    }
   ],
   "source": [
    "from keras.models import Sequential\n",
    "from keras.layers import Dense\n",
    "from keras.layers import LSTM\n",
    "from keras.layers.convolutional import Conv1D\n",
    "from keras.layers.convolutional import MaxPooling1D\n",
    "from keras.layers import Dropout\n",
    "from keras.layers.embeddings import Embedding\n",
    "from keras.preprocessing import sequence\n",
    "\n",
    "# create the model\n",
    "embedding_vecor_length = 8\n",
    "model = Sequential()\n",
    "model.add(Embedding(top_words, embedding_vecor_length, input_length=max_seq_length))\n",
    "model.add(Conv1D(filters=32, kernel_size=3, padding='same', activation='relu'))\n",
    "model.add(Conv1D(filters=32, kernel_size=10, padding='same', activation='relu'))\n",
    "model.add(MaxPooling1D(pool_size=2))\n",
    "model.add(LSTM(100))\n",
    "model.add(Dense(1, activation='sigmoid'))\n",
    "model.compile(loss='binary_crossentropy', optimizer='adam', metrics=['accuracy'])\n",
    "print(model.summary())\n",
    "model.fit(X_train, y_train, epochs=3, batch_size=64)\n",
    "# Final evaluation of the model\n",
    "scores = model.evaluate(X_test, y_test, verbose=0)\n",
    "print(\"Accuracy: %.2f%%\" % (scores[1]*100))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "embedding_13 (Embedding)     (None, 181, 8)            520       \n",
      "_________________________________________________________________\n",
      "conv1d_12 (Conv1D)           (None, 181, 32)           800       \n",
      "_________________________________________________________________\n",
      "conv1d_13 (Conv1D)           (None, 181, 32)           3104      \n",
      "_________________________________________________________________\n",
      "max_pooling1d_5 (MaxPooling1 (None, 90, 32)            0         \n",
      "_________________________________________________________________\n",
      "lstm_11 (LSTM)               (None, 100)               53200     \n",
      "_________________________________________________________________\n",
      "dense_10 (Dense)             (None, 1)                 101       \n",
      "=================================================================\n",
      "Total params: 57,725\n",
      "Trainable params: 57,725\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "None\n",
      "Epoch 1/10\n",
      "15654/15654 [==============================] - 25s 2ms/step - loss: 0.6902 - acc: 0.5277\n",
      "Epoch 2/10\n",
      "15654/15654 [==============================] - 25s 2ms/step - loss: 0.6661 - acc: 0.5960\n",
      "Epoch 3/10\n",
      "15654/15654 [==============================] - 24s 2ms/step - loss: 0.6545 - acc: 0.6239\n",
      "Epoch 4/10\n",
      "15654/15654 [==============================] - 25s 2ms/step - loss: 0.6513 - acc: 0.6202\n",
      "Epoch 5/10\n",
      "15654/15654 [==============================] - 24s 2ms/step - loss: 0.6467 - acc: 0.6303\n",
      "Epoch 6/10\n",
      "15654/15654 [==============================] - 25s 2ms/step - loss: 0.6455 - acc: 0.6306\n",
      "Epoch 7/10\n",
      "15654/15654 [==============================] - 25s 2ms/step - loss: 0.6422 - acc: 0.6380\n",
      "Epoch 8/10\n",
      "15654/15654 [==============================] - 25s 2ms/step - loss: 0.6383 - acc: 0.6365\n",
      "Epoch 9/10\n",
      "15654/15654 [==============================] - 24s 2ms/step - loss: 0.6356 - acc: 0.6422\n",
      "Epoch 10/10\n",
      "15654/15654 [==============================] - 24s 2ms/step - loss: 0.6376 - acc: 0.6380\n",
      "Accuracy: 64.02%\n"
     ]
    }
   ],
   "source": [
    "from keras.models import Sequential\n",
    "from keras.layers import Dense\n",
    "from keras.layers import LSTM\n",
    "from keras.layers.convolutional import Conv1D\n",
    "from keras.layers.convolutional import MaxPooling1D\n",
    "from keras.layers import Dropout\n",
    "from keras.layers.embeddings import Embedding\n",
    "from keras.preprocessing import sequence\n",
    "\n",
    "\n",
    "# create the model\n",
    "embedding_vecor_length = 8\n",
    "model = Sequential()\n",
    "model.add(Embedding(top_words, embedding_vecor_length, input_length=max_seq_length))\n",
    "model.add(Conv1D(filters=32, kernel_size=3, padding='same', activation='relu'))\n",
    "model.add(Conv1D(filters=32, kernel_size=3, padding='same', activation='relu'))\n",
    "model.add(MaxPooling1D(pool_size=2))\n",
    "model.add(LSTM(100, dropout=0.2, recurrent_dropout=0.2))\n",
    "model.add(Dense(1, activation='sigmoid'))\n",
    "model.compile(loss='binary_crossentropy', optimizer='adam', metrics=['accuracy'])\n",
    "print(model.summary())\n",
    "model.fit(X_train, y_train, epochs=10, batch_size=64)\n",
    "# Final evaluation of the model\n",
    "scores = model.evaluate(X_test, y_test, verbose=0)\n",
    "print(\"Accuracy: %.2f%%\" % (scores[1]*100))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Novo Capstone",
   "language": "python",
   "name": "novo"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
