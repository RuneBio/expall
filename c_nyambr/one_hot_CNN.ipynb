{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "# read in high and low expression one_hot files\n",
    "high_exp = pd.concat(\n",
    "    [pd.read_csv(f\"high_exp_one_hot_{i}.csv\", index_col=0)\n",
    "     for i in range(1, 5)]\n",
    ")\n",
    "low_exp = pd.concat(\n",
    "    [pd.read_csv(f\"low_exp_one_hot_{i}.csv\", index_col=0)\n",
    "     for i in range(1, 5)]\n",
    ")\n",
    "\n",
    "# concatenate to form a single dataframe\n",
    "data_df = pd.concat([high_exp, low_exp], axis=0)\n",
    "\n",
    "# function to convert csv files into \n",
    "def string_to_matrix(string):\n",
    "    # convert string to list of one_hot lists\n",
    "    string = str(string)\n",
    "    list_of_strings = string.split('], [')\n",
    "    list_of_lists = [channels.strip().replace('[', '').replace(']', '').replace(',', '').split() \n",
    "                     for channels in list_of_strings\n",
    "                     if 'nan' not in list_of_strings\n",
    "                    ]\n",
    "    # add padding\n",
    "    remaining_pad = 181 - len(list_of_lists)\n",
    "    while remaining_pad > 0:\n",
    "        list_of_lists.append(list([0 for x in range(0, 64)]))\n",
    "        remaining_pad = remaining_pad - 1\n",
    "    # return padded one_hot matrix\n",
    "    return np.array(list_of_lists).astype(np.float)\n",
    "\n",
    "data_df['one_hot_matrix'] = data_df['one_hot_matrix'].apply(string_to_matrix)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create train test split\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "max_len = 181\n",
    "width = 64\n",
    "\n",
    "X = np.zeros((22615, max_len, width))\n",
    "for idx, one_hot_matrix in enumerate(data_df['one_hot_matrix'].values):\n",
    "    X[idx, :, :] = one_hot_matrix\n",
    "\n",
    "    \n",
    "y = data_df['class'].values\n",
    "\n",
    "x_train, x_test, y_train, y_test = train_test_split(\n",
    "    X, y, test_size=0.3, random_state=42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/joshsmith/Git/NovoNordisk_Capstone/.env/lib/python3.6/site-packages/h5py/__init__.py:36: FutureWarning: Conversion of the second argument of issubdtype from `float` to `np.floating` is deprecated. In future, it will be treated as `np.float64 == np.dtype(float).type`.\n",
      "  from ._conv import register_converters as _register_converters\n",
      "Using TensorFlow backend.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From /Users/joshsmith/Git/NovoNordisk_Capstone/.env/lib/python3.6/site-packages/tensorflow/python/util/deprecation.py:497: calling conv1d (from tensorflow.python.ops.nn_ops) with data_format=NHWC is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "`NHWC` for data_format is deprecated, use `NWC` instead\n",
      "Train on 15830 samples, validate on 6785 samples\n",
      "Epoch 1/10\n",
      " - 9s - loss: 0.6985 - acc: 0.5210 - val_loss: 0.6822 - val_acc: 0.5863\n",
      "Epoch 2/10\n",
      " - 8s - loss: 0.6820 - acc: 0.5621 - val_loss: 0.6750 - val_acc: 0.5965\n",
      "Epoch 3/10\n",
      " - 8s - loss: 0.6689 - acc: 0.5942 - val_loss: 0.6674 - val_acc: 0.6122\n",
      "Epoch 4/10\n",
      " - 9s - loss: 0.6568 - acc: 0.6121 - val_loss: 0.6619 - val_acc: 0.6141\n",
      "Epoch 5/10\n",
      " - 9s - loss: 0.6450 - acc: 0.6293 - val_loss: 0.6603 - val_acc: 0.6074\n",
      "Epoch 6/10\n",
      " - 8s - loss: 0.6355 - acc: 0.6417 - val_loss: 0.6545 - val_acc: 0.6178\n",
      "Epoch 7/10\n",
      " - 9s - loss: 0.6221 - acc: 0.6591 - val_loss: 0.6519 - val_acc: 0.6180\n",
      "Epoch 8/10\n",
      " - 9s - loss: 0.6154 - acc: 0.6677 - val_loss: 0.6492 - val_acc: 0.6203\n",
      "Epoch 9/10\n",
      " - 9s - loss: 0.6058 - acc: 0.6728 - val_loss: 0.6512 - val_acc: 0.6196\n",
      "Epoch 10/10\n",
      " - 9s - loss: 0.5988 - acc: 0.6845 - val_loss: 0.6518 - val_acc: 0.6134\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x10ad1aeb8>"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# simple model per Yoon Kim (2014)\n",
    "\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Dense, Dropout, Flatten\n",
    "from keras.layers import Conv1D, GlobalMaxPooling1D\n",
    "\n",
    "model = Sequential()\n",
    "model.add(Conv1D(100, 3, activation='relu', input_shape=(181, 64)))\n",
    "model.add(GlobalMaxPooling1D())\n",
    "# model.add(Flatten())\n",
    "# model.add(Dense(32))\n",
    "model.add(Dropout(0.5))\n",
    "model.add(Dense(1, activation='sigmoid'))\n",
    "\n",
    "model.compile(loss='binary_crossentropy',\n",
    "              optimizer='adam',\n",
    "              metrics=['accuracy'])\n",
    "\n",
    "model.fit(x_train, y_train, batch_size=50, epochs=10,\n",
    "          validation_data=(x_test, y_test), verbose=2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Now trying to use an example based on Kim's paper.\n",
    "# adapted from https://github.com/alexander-rakhlin/CNN-for-Sentence-Classification-in-Keras/blob/master/sentiment_cnn.py"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 15830 samples, validate on 6785 samples\n",
      "Epoch 1/10\n",
      " - 9s - loss: 0.7368 - acc: 0.5057 - val_loss: 0.6908 - val_acc: 0.5422\n",
      "Epoch 2/10\n",
      " - 7s - loss: 0.6974 - acc: 0.5190 - val_loss: 0.6889 - val_acc: 0.5761\n",
      "Epoch 3/10\n",
      " - 7s - loss: 0.6894 - acc: 0.5331 - val_loss: 0.6875 - val_acc: 0.5688\n",
      "Epoch 4/10\n",
      " - 7s - loss: 0.6859 - acc: 0.5427 - val_loss: 0.6855 - val_acc: 0.5826\n",
      "Epoch 5/10\n",
      " - 7s - loss: 0.6843 - acc: 0.5519 - val_loss: 0.6837 - val_acc: 0.5951\n",
      "Epoch 6/10\n",
      " - 7s - loss: 0.6815 - acc: 0.5630 - val_loss: 0.6814 - val_acc: 0.6022\n",
      "Epoch 7/10\n",
      " - 7s - loss: 0.6800 - acc: 0.5672 - val_loss: 0.6792 - val_acc: 0.6041\n",
      "Epoch 8/10\n",
      " - 7s - loss: 0.6750 - acc: 0.5768 - val_loss: 0.6764 - val_acc: 0.6088\n",
      "Epoch 9/10\n",
      " - 7s - loss: 0.6729 - acc: 0.5826 - val_loss: 0.6740 - val_acc: 0.6091\n",
      "Epoch 10/10\n",
      " - 7s - loss: 0.6708 - acc: 0.5900 - val_loss: 0.6724 - val_acc: 0.6115\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x214ea1080>"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from keras.models import Sequential, Model\n",
    "from keras.layers import Dense, Dropout, Input, MaxPooling1D, Convolution1D, Flatten\n",
    "from keras.layers.merge import Concatenate\n",
    "\n",
    "filter_sizes = (3, 4, 5)\n",
    "num_filters = 10\n",
    "dropout_prob = (0.5, 0.8)\n",
    "hidden_dims = 50\n",
    "\n",
    "# prepare input shape\n",
    "input_shape = (181, 64)\n",
    "model_input = Input(shape=input_shape)\n",
    "z = model_input\n",
    "\n",
    "# z = Dropout(dropout_prob[0])(z)\n",
    "\n",
    "# Convolutional ddddblock\n",
    "conv_blocks = []\n",
    "for sz in filter_sizes:\n",
    "    conv = Convolution1D(filters=num_filters,\n",
    "                         kernel_size=sz,\n",
    "                         padding=\"valid\",\n",
    "                         activation=\"relu\",\n",
    "                         strides=1)(z)\n",
    "    conv = GlobalMaxPooling1D()(conv)\n",
    "    conv_blocks.append(conv)\n",
    "z = Concatenate()(conv_blocks) if len(conv_blocks) > 1 else conv_blocks[0]\n",
    "# z = Flatten()(z)\n",
    "z = Dropout(dropout_prob[1])(z)\n",
    "# z = Dense(hidden_dims, activation=\"selu\")(z)\n",
    "model_output = Dense(1, activation=\"sigmoid\")(z)\n",
    "\n",
    "model = Model(model_input, model_output)\n",
    "model.compile(loss=\"binary_crossentropy\", optimizer=\"adam\", metrics=[\"accuracy\"])\n",
    "\n",
    "# Train the model\n",
    "model.fit(x_train, y_train, batch_size=50, epochs=10,\n",
    "          validation_data=(x_test, y_test), verbose=2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### the following architecture comes from the keras docs... seems to overfit"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 15830 samples, validate on 6785 samples\n",
      "Epoch 1/10\n",
      " - 13s - loss: 0.6888 - acc: 0.5275 - val_loss: 0.6705 - val_acc: 0.5876\n",
      "Epoch 2/10\n",
      " - 12s - loss: 0.6568 - acc: 0.6089 - val_loss: 0.6462 - val_acc: 0.6318\n",
      "Epoch 3/10\n",
      " - 14s - loss: 0.6470 - acc: 0.6318 - val_loss: 0.6423 - val_acc: 0.6335\n",
      "Epoch 4/10\n",
      " - 13s - loss: 0.6381 - acc: 0.6372 - val_loss: 0.6391 - val_acc: 0.6321\n",
      "Epoch 5/10\n",
      " - 13s - loss: 0.6274 - acc: 0.6483 - val_loss: 0.6343 - val_acc: 0.6370\n",
      "Epoch 6/10\n",
      " - 14s - loss: 0.6152 - acc: 0.6653 - val_loss: 0.6348 - val_acc: 0.6327\n",
      "Epoch 7/10\n",
      " - 12s - loss: 0.5949 - acc: 0.6810 - val_loss: 0.6438 - val_acc: 0.6248\n",
      "Epoch 8/10\n",
      " - 13s - loss: 0.5709 - acc: 0.7025 - val_loss: 0.6346 - val_acc: 0.6323\n",
      "Epoch 9/10\n",
      " - 13s - loss: 0.5453 - acc: 0.7233 - val_loss: 0.6692 - val_acc: 0.6327\n",
      "Epoch 10/10\n",
      " - 12s - loss: 0.5106 - acc: 0.7488 - val_loss: 0.6699 - val_acc: 0.6246\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x10bd676d8>"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from keras.models import Sequential\n",
    "from keras.layers import Dense, Dropout\n",
    "from keras.layers import Embedding\n",
    "from keras.layers import Conv1D, GlobalAveragePooling1D, MaxPooling1D\n",
    "\n",
    "model = Sequential()\n",
    "model.add(Conv1D(64, 3, activation='relu', input_shape=(181, 64)))\n",
    "model.add(Conv1D(64, 3, activation='relu'))\n",
    "model.add(MaxPooling1D(3))\n",
    "model.add(Conv1D(128, 3, activation='relu'))\n",
    "model.add(Conv1D(128, 3, activation='relu'))\n",
    "model.add(GlobalAveragePooling1D())\n",
    "model.add(Dropout(0.8))\n",
    "model.add(Dense(1, activation='sigmoid'))\n",
    "\n",
    "model.compile(loss='binary_crossentropy',\n",
    "              optimizer='adam',\n",
    "              metrics=['accuracy'])\n",
    "\n",
    "model.fit(x_train, y_train, batch_size=50, epochs=10,\n",
    "          validation_data=(x_test, y_test), verbose=2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Novo Capstone",
   "language": "python",
   "name": "novo"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
